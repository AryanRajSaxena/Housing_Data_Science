{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError, ReadTimeout\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchAndSaveToFile(url, path, retries=3, backoff_factor=1):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Increased timeout from 10 to 20 seconds\n",
    "            r = requests.get(url, headers=headers, timeout=20)\n",
    "            with open(path, \"w\", encoding='utf-8') as f:\n",
    "                f.write(r.text)\n",
    "            break  # Success, exit the loop\n",
    "        except (ConnectionError, ReadTimeout) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            sleep(backoff_factor * (2 ** attempt))  # Exponential backoff\n",
    "    else:  # All attempts failed\n",
    "        print(f\"Failed to fetch data after {retries} attempts.\")\n",
    "\n",
    "\n",
    "def beauty(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding='utf-8') as f:\n",
    "            html_doc = f.read()\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            new_content = soup.prettify()\n",
    "        with open(\"new_content.html\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(new_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "price_list = []; emi_list = []; img_list = []; appartment_name_list = []; appartment_by_list = []; size_list = []; details_list=[]; desc_list = []; places_list = []\n",
    "\n",
    "\n",
    "for p in range(1, 173):\n",
    "    url = \"https://housing.com/buy-apartments-in-jaipur-for-sale-srpid-M1P268dw8h2rtw7ykpy?page=\" + str(p)\n",
    "\n",
    "    fetchAndSaveToFile(url, \"content.html\")\n",
    "    beauty(\"content.html\")\n",
    "    with open(R\"D:\\Housing_DSc\\Housing_Data_Science\\housing_data_science\\notebooks\\new_content.html\",\"r\", encoding=\"utf-8\") as file:\n",
    "        html_doc = file.read()\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    # print(soup.title)\n",
    "    article_tag = soup.findAll('article', attrs={'data-testid': 'card-container'})\n",
    "    # print(script_tag)\n",
    "\n",
    "    for k in range(0,len(article_tag)):\n",
    "\n",
    "        try:\n",
    "            price = article_tag[k].findAll('div', attrs={'data-testid': 'priceid'})\n",
    "            price_list.append(price[0].text.strip())\n",
    "        except:\n",
    "            price_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            emi = article_tag[k].findAll('span', attrs={'class': '_9jtlke _l881bl _gzftgi _7l1994 _c81fwx _g3dlk8 T_9671ecd9'})\n",
    "            emi_list.append(emi[0].text.strip())\n",
    "        except:\n",
    "            emi_list.append(None)\n",
    "\n",
    "        try:\n",
    "            img = article_tag[k].find('div', attrs={'class': 'css-uwwqev'})\n",
    "            img_list.append(img.img['src'])\n",
    "        except Exception as e:\n",
    "            img_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            appartment_name = article_tag[k].findAll('div', attrs={'class': '_9s1txw _gqyh40 _0h1q9y T_8c8ed98f'})\n",
    "            appartment_name_list.append(appartment_name[0].text.strip().split(\"\\n\")[0])\n",
    "        except:\n",
    "            appartment_name_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            appartment_by = article_tag[k].findAll('div', attrs={'class': '_c81fwx _g3dlk8 _7l1v10 T_1bbfaea9'})\n",
    "            appartment_by_list.append(appartment_by[0].text.strip().split(\"\\n\")[0])\n",
    "        except:\n",
    "            appartment_by_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            size = article_tag[k].findAll('h3', attrs={'class': 'T_091c165f _sq1l2s _vv1q9c _ks15vq T_79c363f6 _5vy24jg8 _blas14la _csbfng _g3dlk8 _c81fwx _h3ftgi'})\n",
    "            size_list.append(\" \".join(size[0].text.strip().split()))\n",
    "        except:\n",
    "            size_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            details = article_tag[k].findAll('section', attrs={'class': 'T_ea8ae345 _1enfn7od _12el1ule _12il1osq _11ar1l2s _1vd01q9c _orqr15vq _cbben7od _3nng1e54 _12eccj1k _dgd0cs5v _gcpm15vq _9j73ad _gnftgi _9s1txw'})\n",
    "            details_list.append(\" \".join(details[0].text.strip().split()))\n",
    "        except:\n",
    "            details_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            desc = article_tag[k].findAll('div', attrs={'data-q': 'desc'})\n",
    "            desc_list.append(\" \".join(desc[0].text.split()[:-1]))\n",
    "        except:\n",
    "            desc_list.append(None)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            places = article_tag[k].findAll('div', attrs={'class': 'T_aa486ccc _be1g80 _n5ftgi _0h1h6o _9s1txw _h3exct _84ftgi'})\n",
    "            places_list.append(\" \".join(places[0].text.split()[:-1]))\n",
    "        except:\n",
    "            places_list.append(None)\n",
    "    print(\"Page ->\",p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "    \"Price\": price_list,\n",
    "    \"EMI\": emi_list,\n",
    "    \"Image\": img_list,\n",
    "    \"Appartment Name\": appartment_name_list,\n",
    "    \"Appartment By\": appartment_by_list,\n",
    "    \"Size\": size_list,\n",
    "    \"Details\": details_list,\n",
    "    \"Description\": desc_list,\n",
    "    \"Places\": places_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dic).to_csv(\"housing_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
